[Back](./readme.md)
# Philosophy of Artificial Intelligence
###### Lecture 3 - 2023-01-23
**Weak AI** Can machines act as if they were intelligent?

**Strong AI** Are machines that do so actually intelligent as opposed to simulating intelligence?

## Weak AI hypothesis

> "Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it."

### Argument from disability

A machine can never
- be kind
- resourceful
- beautiful
- friendly
- have initiative
- have a sense of humour
- tell right from wrong
- make mistakes
- fall in love
- have as much diversity of behaviour as machines
- do something really new

### The mathematically objection
- Certain mathematical questions are in principle unanswerable by a formal system (Godel's incompleteness theorem)
- Lucal 1961 "This theorem shows that machines are metnally inferior to humans, becaues machines are formal systems that are limited by the incompleteness theorem while humans have no such limitation."

### The argument from informality
> “Human behaviour is far too complex to be captured by any simple set of rules and that because computers can do no more than follow set of rules, they cannot generate behaviour as intelligent as that of humans”.

## Strong AI hypothesis (Searle, 1980)
> "The appropriately programmed computer with the right inputs and outputs woul thereby have a mind in exactly the same sense human beings have minds."

A machine that passes the Turing Test would still not actually be thining, it would be only a simulation of thinking.

## Simulation vs Implementation
> Searle (1980): “No one supposes that a computer simulation of a storm will leave us all wet… Why on earth would anyone in his right mind suppose a computer simulation of mental processes actually had mental processes”?

A computer simulation of an addition is an addition. Are mental processes more like storms or additions?

## Functionalism vs Biological Naturalism
**Functionalism:** any two systems with isomorphic casual processes have the same mental states. A computer could have the same mental states of a person.

**Biological Naturalism:** mental states are high-level emergent features that are caused by low-level neurological processes in the neurons, and it is the unspecified properties of neurons that matter. A computer must run on a neural architecture to have the same mental states of a person.

## The Mind-Body Problem: Dualism vs Materialism
Mind-body problem: how mental states and processes are related to brain states and processess?

**Dualism** (Descartes, 1641) mental processes are not physical processes.
- Immortal soul
- Free will
- Consciousness

**Materialism:** mental states are physical states (thoughts, pain, emotions are not outside this world). Brains cause minds.

## Neurobiology

**Head Magnetic Resonance Imaging:** can study electric activity of alive brain when the subject thinks consciously of something.

**Brain-computer interface:** your thought can be used to control a computer.

**Free will:** physical states cause mental states.

**Libet experiemnt:** decisions are subconscious and are registered at conscious level with some delay. Is free will (conscious act of decision) an illusion?


**Experiment:** Replace gradually all neurons one neuron at the time with electronic devices with the same inputoutput behaviour and same connectivity in an alive conscious human. Is the subject’s external behaviour changed after the experiment? Is the subject’s subjective experience changed after the experiment?

**Ethics and Risks**
- lose their jobs to automation
- too much leisure time
- lose their sense of being unique
- lose their privacy rights
- AI systems might result in a loss of accountability
- Mass surveillance

## Friendly AI
> Bostrom: “We should assume that a 'superintelligence' would be able to achieve whatever goals it has. Therefore, it is extremely important that the goals we endow it with, and its entire motivation system, is 'human friendly.'”

### The Three Laws of Robotics (Asimov):
– A robot may not injure a human being or, through inaction, allow a human being to come to harm.
– A robot must obey the orders given it by human beings, except where such orders would conflict with the First Law.
– A robot must protect its own existence as long as such protection does not conflict with the First or Second Law.


